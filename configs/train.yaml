defaults:
  - _self_
  - data_train:
    - mimic_cxr
    - chexpert
  - data_valid:
    - mimic_cxr
    - chexpert
  - dataloader: dataloader_128
  - tokenizer: clinical_bert
  - transform: clahe
  - model: clip_resnet_clinical # clip_swin_clinical # mainly for training
  - resnet: clip_resnet_clinical
  - swin: clip_swin_clinical
  - optimizer: adamw
  - scheduler: cosine_epoch15_warmup1
  - loss: cxr_clip

base:
  seed: 1234
  amp : True
  image_size: 224
  text_max_length: 256
  loss_best: contrastive
  data_frac: 1.0
  output:
    checkpoint: ${hydra:run.dir}/checkpoints/
    tensorboard: ${hydra:run.dir}/tensorboard/

training_params:
  batch_size: 360
  num_workers: 10
  batch_style: experiment
  train_from_scratch: false
  epoch_based_patience: 25
  iteration_evaluate_frequency: 50
  text_cl_weight: 1.0
  ct_cl_weight: 1.0
  learning_rate: 5e-5
  weight_decay: 1e-4
  epochs: 202
  min_epochs: 202
  training_pretrain_baseline: ''
  use_pretrained_xray_encoder: true

linear_probing_params:
  baseline_type: 'cxr_clip_resnet' # 'modeltype_Resnet__batchstyle_experiment__bs_360__lr_5e-05__wd_0.0001__textcl_1.0__ctcl_1.0__pretrained_True_50_epoch' #'modeltype_Swin__batchstyle_experiment__bs_360__lr_5e-05__wd_0.0001__textcl_1.0__ctcl_1.0__pretrained_True_50_epoch' #'modeltype_Swin__batchstyle_experiment__bs_360__lr_5e-05__wd_0.0001__textcl_1.0__ctcl_1.0__pretrained_True_50_epoch' #'modeltype_Swin__batchstyle_experiment__bs_360__lr_5e-05__wd_0.0001__textcl_1.0__ctcl_1.0__pretrained_True_50_epoch'
  is_linear_probe_eval: true
  multi_sweep_evaluation: false
  num_epochs: 1 # 1000
  patience: 20
  batch_size: 4096
  learning_rate: 1e-2 # 1e-3
  progress_window: 2
  num_workers: 10
  train_data_portion: 0.1
  evaluation_dataset: 'mimic' # internal, mimic, vinBig
  cpt_dest: '/cluster/projects/mcintoshgroup/CT-RATE-CHECKPOINTS/linear_probe_evaluation'

zero_shot_params:
  baseline_type: '' # 'modeltype_Swin__batchstyle_experiment__bs_360__lr_5e-05__wd_0.0001__textcl_1.0__ctcl_1.0__pretrained_True_50_epoch' #'modeltype_Swin__batchstyle_experiment__bs_360__lr_5e-05__wd_0.0001__textcl_1.0__ctcl_1.0__pretrained_True_50_epoch' #'modeltype_Swin__batchstyle_experiment__bs_360__lr_5e-05__wd_0.0001__textcl_1.0__ctcl_1.0__pretrained_True_50_epoch' #'modeltype_Swin__batchstyle_experiment__bs_360__lr_5e-05__wd_0.0001__textcl_1.0__ctcl_1.0__pretrained_True_50_epoch' # just like the one in linear probe with additional option as ct_clip
  test_bed: 'internal_ct_val' # the dataset that the vision and text encoders suppose to run on.
  num_workers: 10
  batch_size: 1024

xray_feature_caching_params:
  baseline_type: '' # 'modeltype_Swin__batchstyle_experiment__bs_360__lr_5e-05__wd_0.0001__textcl_1.0__ctcl_1.0__pretrained_True_50_epoch' #'modeltype_Swin__batchstyle_experiment__bs_360__lr_5e-05__wd_0.0001__textcl_1.0__ctcl_1.0__pretrained_True_50_epoch' #'modeltype_Swin__batchstyle_experiment__bs_360__lr_5e-05__wd_0.0001__textcl_1.0__ctcl_1.0__pretrained_True_50_epoch' #'modeltype_Swin__batchstyle_experiment__bs_360__lr_5e-05__wd_0.0001__textcl_1.0__ctcl_1.0__pretrained_True_50_epoch' # just like the one in linear probe with additional option as ct_clip
  num_workers: 10
  batch_size: 1024

internal_split_caching_params:
  baseline_type: '' # 'modeltype_Swin__batchstyle_experiment__bs_360__lr_5e-05__wd_0.0001__textcl_1.0__ctcl_1.0__pretrained_True_50_epoch' #'modeltype_Swin__batchstyle_experiment__bs_360__lr_5e-05__wd_0.0001__textcl_1.0__ctcl_1.0__pretrained_True_50_epoch' #'modeltype_Swin__batchstyle_experiment__bs_360__lr_5e-05__wd_0.0001__textcl_1.0__ctcl_1.0__pretrained_True_50_epoch' #'modeltype_Swin__batchstyle_experiment__bs_360__lr_5e-05__wd_0.0001__textcl_1.0__ctcl_1.0__pretrained_True_50_epoch' # just like the one in linear probe with additional option as ct_clip
  num_workers: 10
  batch_size: 1024
  train_data_portion: 0.1
  evaluation_dataset: 'mimic' # default

hydra:
  run:
    dir: ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
